# Plan: 4
# Generated by: discovery:4.d
# Timestamp: 2025-12-17T16:30:00Z

story_id: "4"
title: "Validation - End-to-End UX Test"
created: "2025-12-17T16:30:00Z"
created_by: "discovery:4.d"

# Analysis Summary
analysis:
  summary: "Create comprehensive E2E validation test suite for daemon auto-enable UX, covering fresh install, error feedback, reinstall scenarios, and cross-platform compatibility"
  complexity: "medium"
  estimated_files: 3
  estimated_tokens: 15000
  risk_factors:
    - "Platform-specific behavior may require conditional test logic"
    - "Fresh install tests require cleanup to avoid state pollution"
    - "MCP connection testing may be flaky without proper retry logic"
    - "Test timing (5 second threshold) may vary across systems"

# Files to Create
files_to_create:
  - path: "mcp_server/tests/test_daemon_e2e_validation.py"
    purpose: "End-to-end validation tests for daemon UX flows (fresh install, error feedback, reinstall)"
    pattern_source: "mcp_server/tests/test_comprehensive_integration.py"
    estimated_lines: 350

# Files to Modify
files_to_modify:
  - path: "mcp_server/tests/run_tests.py"
    changes:
      - "Add 'e2e' test suite option to test runner"
      - "Add command-line option for running daemon E2E tests"
      - "Add suite-specific pytest arguments for e2e marker"
    lines_affected: 25

  - path: "mcp_server/tests/pytest.ini"
    changes:
      - "Add 'e2e' marker to markers section"
      - "Document e2e marker usage"
    lines_affected: 5

# Integration Points
integration:
  - file: "mcp_server/tests/run_tests.py"
    type: "suite registration"
    description: "Register 'e2e' suite in test runner with appropriate pytest args"

  - file: "mcp_server/tests/pytest.ini"
    type: "marker definition"
    description: "Define e2e marker for categorizing end-to-end tests"

  - file: "mcp_server/daemon/manager.py"
    type: "external dependency"
    description: "Tests will invoke daemon install/uninstall via DaemonManager"

  - file: "mcp_server/unified_config.py"
    type: "external dependency"
    description: "Tests will read/modify graphiti.config.json via unified config"

# Patterns to Follow
patterns:
  - source: "mcp_server/tests/test_comprehensive_integration.py"
    pattern: "Use pytest fixtures for setup/teardown, async test methods, proper error handling"

  - source: "mcp_server/tests/test_fixtures.py"
    pattern: "Define reusable fixtures for config manipulation, daemon state management"

  - source: "mcp_server/tests/run_tests.py"
    pattern: "Test suite registration with conditional execution based on markers"

  - source: "docs/TROUBLESHOOTING_DAEMON.md"
    pattern: "Expected error messages and troubleshooting flows for validation assertions"

# Test Requirements
test_requirements:
  integration:
    - "test_fresh_install_flow: Uninstall daemon → Install → Verify config created with enabled=true → Verify MCP server starts within 5s → Verify health endpoint accessible → Verify Claude Code MCP tools work"
    - "test_error_messages_without_daemon: Set daemon.enabled=false → Attempt MCP tool call → Verify error message is actionable (guides user to enable daemon)"
    - "test_reinstall_idempotent: With daemon running → Run install again → Verify no errors → Verify daemon continues running → Verify config preserved"
    - "test_cross_platform_validation: Detect platform (Windows/Linux/macOS) → Run platform-specific validations → Document platform status in test output"
    - "test_health_check_timing: After install → Poll health endpoint → Assert response within 5 seconds → Verify health check returns 200 OK"
    - "test_mcp_connection_reliability: After install → Retry MCP tool calls with exponential backoff → Verify connection succeeds → Assert tools return expected data"
    - "test_config_auto_enable: After install → Read graphiti.config.json → Assert daemon.enabled=true → Assert default host/port configured"
    - "test_daemon_status_command: After install → Run 'graphiti-mcp daemon status' → Parse output → Verify shows 'running' state"

  unit:
    - "test_platform_detection: Mock platform.system() → Test detection logic → Verify correct platform string returned"
    - "test_config_cleanup_fixture: Create test config → Run fixture cleanup → Verify config removed or restored"
    - "test_daemon_state_fixture: Mock daemon state → Test fixture setup/teardown → Verify state correctly managed"

  security:
    - "N/A - Validation story, no security-critical functionality"

# Acceptance Criteria Mapping
acceptance_criteria:
  - id: "AC-4.1"
    text: "Fresh install flow works: daemon install → MCP server running → Claude Code connects"
    implementation: "files_to_create[0] (test_fresh_install_flow)"
    tests: "test_requirements.integration[0]"

  - id: "AC-4.2"
    text: "Error messages appear correctly when daemon not installed"
    implementation: "files_to_create[0] (test_error_messages_without_daemon)"
    tests: "test_requirements.integration[1]"

  - id: "AC-4.3"
    text: "Reinstall scenario works (daemon already installed, run install again)"
    implementation: "files_to_create[0] (test_reinstall_idempotent)"
    tests: "test_requirements.integration[2]"

  - id: "AC-4.4"
    text: "Cross-platform validation (Windows focus, document Linux/macOS status)"
    implementation: "files_to_create[0] (test_cross_platform_validation)"
    tests: "test_requirements.integration[3]"

# Implementation Notes
implementation_notes:
  - "Use pytest fixtures for daemon state management (setup/teardown)"
  - "Fresh install tests MUST clean up state before/after to avoid pollution"
  - "MCP connection tests should use retry logic with exponential backoff for reliability"
  - "Platform detection should use platform.system() and conditionally skip unsupported tests"
  - "Health check timing should use time.time() polling with 5 second timeout"
  - "Config modifications should be isolated per test (use temporary config paths or fixtures)"
  - "All daemon operations should be wrapped in try/finally for cleanup"
  - "Tests should verify both technical correctness (daemon running) and UX quality (error messages actionable)"

# Dependencies
dependencies:
  - story_id: "1"
    description: "Auto-enable daemon functionality must be complete"
    reason: "Fresh install tests validate auto-enable behavior"

  - story_id: "2"
    description: "Error feedback mechanisms must be complete"
    reason: "Error message tests validate feedback quality"

  - story_id: "3"
    description: "Installation documentation must be complete"
    reason: "Tests validate documented flows match actual behavior"

# Token Budget Breakdown
token_budget:
  discovery: 12200
  implementation: 25000
  testing: 15000
  total: 52200
  buffer: 7800
  max: 60000
