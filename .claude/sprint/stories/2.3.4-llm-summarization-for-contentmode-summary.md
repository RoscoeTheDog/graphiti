# Story 2.3.4: LLM Summarization for ContentMode.SUMMARY

**Status**: unassigned
**Parent**: Story 2.3
**Depends on**: Story 2.3.3
**Created**: 2025-11-18
**Priority**: Low (enhancement, non-default config)

## Context

Story 2.3 implemented the configurable filtering system with ContentMode enum (FULL, SUMMARY, OMIT). However, LLM-based summarization for `ContentMode.SUMMARY` was not implemented - only hardcoded 1-line summaries for tool results exist.

**Current Implementation:**
- ✅ ContentMode.SUMMARY works for tool results (uses existing 1-line summaries from Story 2.2)
- ❌ ContentMode.SUMMARY for user messages falls back to FULL (TODO comment at filter.py:185)
- ❌ ContentMode.SUMMARY for agent messages falls back to FULL (TODO comment at filter.py:185)

**Default Config:** Uses `ContentMode.FULL` for user/agent messages, so this missing feature doesn't affect typical usage.

**Why This is an Enhancement:**
- Default configuration is fully functional (50%+ token reduction via tool result summaries)
- LLM summarization only needed for non-default configs (users who explicitly choose SUMMARY mode for messages)
- Adds cost (~$0.01-0.05 per message) and latency (200-500ms per summary)

## Who

**Primary**: Advanced users who want aggressive token reduction
**Secondary**: Users with very long conversations who need maximum efficiency

## What

Implement LLM-based content summarization for user and agent messages when `ContentMode.SUMMARY` is configured.

**Scope:**
1. Create `MessageSummarizer` class (reuse Graphiti LLM client)
2. Integrate into `SessionFilter.filter_messages()` for user/agent message handling
3. Add caching to avoid re-summarizing identical content
4. Make summarization opt-in (default remains ContentMode.FULL)

**Out of Scope:**
- Changing default configuration (stays FULL for user/agent messages)
- Tool result summarization (already implemented via hardcoded summaries)
- Session-level summarization (deprecated, Graphiti handles this)

## Why

**Use Cases:**
1. **Very long sessions**: Users with 100+ message conversations need maximum token reduction
2. **Cost-conscious users**: Users willing to pay LLM cost upfront to reduce Graphiti storage costs
3. **Privacy-focused users**: Users who want condensed summaries instead of full message content

**Benefits:**
- Additional 15-25% token reduction (beyond current 35% from tool summaries)
- Configurable tradeoff: users choose between fidelity (FULL) vs efficiency (SUMMARY)
- Completes ContentMode.SUMMARY implementation (removes TODO comments)

**Risks:**
- Adds LLM cost: ~$0.01-0.05 per message (gpt-4.1-mini)
- Adds latency: 200-500ms per message summarization
- May lose nuance/context in aggressive summarization

## Where

**New File:**
- `graphiti_core/session_tracking/message_summarizer.py` (~150 LOC)

**Modified Files:**
- `graphiti_core/session_tracking/filter.py` (integrate MessageSummarizer)
- `mcp_server/unified_config.py` (add summarizer config options)

**Test Files:**
- `tests/session_tracking/test_message_summarizer.py` (new, 12+ tests)
- `tests/session_tracking/test_filter_config.py` (update existing tests)

## When

**Timing:** After Story 2.3.3 completion, before Story 5 (CLI Integration)

**Duration:** ~1.5-2 hours
- Implementation: 1 hour (MessageSummarizer class + integration)
- Testing: 30 minutes (unit tests + integration tests)
- Documentation: 15 minutes (CONFIGURATION.md updates)

**Dependencies:**
- ✅ Story 2.3.3 completed (config validator working)
- ✅ Graphiti LLM client available (reuse existing client)

## Acceptance Criteria

### Implementation
- [ ] `MessageSummarizer` class created in `message_summarizer.py`
  - [ ] Reuses Graphiti LLM client (no new API keys needed)
  - [ ] Uses gpt-4.1-mini for cost efficiency (~$0.15 per 1M tokens)
  - [ ] Prompt template: "Summarize this message concisely in 1-2 sentences, preserving key intent and context."
  - [ ] Handles errors gracefully (fallback to FULL on LLM failure)
  - [ ] Caching layer to avoid re-summarizing identical content
- [ ] `SessionFilter` integration
  - [ ] User message handling: Call MessageSummarizer when `ContentMode.SUMMARY` configured
  - [ ] Agent message handling: Call MessageSummarizer when `ContentMode.SUMMARY` configured
  - [ ] Remove TODO comments from filter.py:185
  - [ ] Backward compatible (default behavior unchanged)
- [ ] Configuration options (optional, in unified_config.py)
  - [ ] `summarizer.enabled: bool` (default: true)
  - [ ] `summarizer.model: str` (default: "gpt-4.1-mini")
  - [ ] `summarizer.max_length: int` (default: 200 characters)
  - [ ] `summarizer.cache_ttl: int` (default: 3600 seconds)

### Testing
- [ ] Unit tests for MessageSummarizer (12+ tests)
  - [ ] Test summarization produces concise output
  - [ ] Test caching works (same input → cached result)
  - [ ] Test error handling (LLM failure → fallback to FULL)
  - [ ] Test different message types (user vs agent)
  - [ ] Test max_length enforcement
- [ ] Integration tests with SessionFilter
  - [ ] Test SUMMARY mode for user messages calls summarizer
  - [ ] Test SUMMARY mode for agent messages calls summarizer
  - [ ] Test FULL mode bypasses summarizer (no LLM calls)
  - [ ] Test OMIT mode bypasses summarizer
- [ ] Performance tests
  - [ ] Measure latency (target: <500ms per message)
  - [ ] Measure cost (target: <$0.05 per message)
- [ ] Test coverage >80%

### Documentation
- [ ] Update CONFIGURATION.md "Filtering Configuration" section
  - [ ] Document `summarizer.*` config fields
  - [ ] Add example: Aggressive SUMMARY mode with cost estimates
  - [ ] Add warning: LLM cost and latency tradeoffs
- [ ] Add docstrings to MessageSummarizer class
- [ ] Update filter.py comments (remove TODOs, add "Uses MessageSummarizer" notes)

### Cross-Cutting Requirements
- [ ] **Platform-Agnostic**: No platform-specific code (LLM client is cross-platform)
- [ ] **Error Handling**: Graceful fallback to FULL on LLM errors, comprehensive logging
- [ ] **Type Safety**: Type hints for all functions, Pydantic models for config
- [ ] **Testing**: >80% coverage with edge cases (LLM failures, cache hits/misses)
- [ ] **Performance**: <500ms latency, <5% overhead when caching is effective
- [ ] **Security**: No sensitive data in prompts, obfuscate API keys in logs
- [ ] **Configuration**: Use unified `graphiti.config.json` system
- [ ] **Documentation**: User guide for enabling SUMMARY mode

## Implementation Notes

### Reuse Graphiti LLM Client
Story 4 refactoring removed SessionSummarizer, but we can reuse Graphiti's LLM client:

```python
from graphiti_core.llm_client import LLMClient

class MessageSummarizer:
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.cache = {}  # Simple in-memory cache

    async def summarize(self, content: str, max_length: int = 200) -> str:
        """Summarize message content using LLM."""
        # Check cache
        cache_key = hash(content)
        if cache_key in self.cache:
            return self.cache[cache_key]

        # Call LLM
        prompt = f"Summarize this message concisely in 1-2 sentences: {content}"
        summary = await self.llm_client.generate_text(prompt, max_tokens=50)

        # Cache result
        self.cache[cache_key] = summary[:max_length]
        return self.cache[cache_key]
```

### Integration Point in filter.py

Replace TODO comments (line 185):
```python
elif self.config.agent_messages == ContentMode.SUMMARY:
    # Use MessageSummarizer for LLM-based summarization
    if self.summarizer and self.summarizer.enabled:
        content = await self.summarizer.summarize(message.content)
    else:
        content = message.content  # Fallback to FULL
```

### Cost Analysis

**Default Config (SUMMARY for tools only):**
- Tool results: Hardcoded summaries (free)
- User/agent messages: FULL (no LLM cost)
- **Total cost:** $0/session

**Aggressive Config (SUMMARY for all):**
- Tool results: Hardcoded summaries (free)
- User messages: LLM summarization (~5 messages × $0.01 = $0.05)
- Agent messages: LLM summarization (~5 messages × $0.01 = $0.05)
- **Total cost:** ~$0.10/session (3x increase)

**Tradeoff:** Users choose between free FULL mode vs paid SUMMARY mode for messages.

## Success Criteria

1. **Functional**: ContentMode.SUMMARY works for all message types (no fallback to FULL)
2. **Performance**: <500ms latency per message, <$0.05 cost per message
3. **Quality**: Summaries preserve key intent and context (manual review of 10 samples)
4. **Backward Compatible**: Default config unchanged, existing tests pass
5. **Test Coverage**: >80% coverage, all edge cases handled

## Questions for User

1. **Priority**: Is this enhancement needed before Story 5 (CLI Integration)?
2. **Cost**: Are users willing to pay ~$0.10/session for aggressive summarization?
3. **Model Choice**: Use gpt-4.1-mini (cheap) or gpt-4o-mini (better quality)?

## Related Stories

- **Story 2.2**: Tool Output Summarization (hardcoded summaries, no LLM)
- **Story 2.3**: Configurable Filtering System (ContentMode enum, infrastructure)
- **Story 4.1**: Session Summarizer (deprecated, session-level not message-level)
- **Story 2.3.3**: Configuration Validator (prerequisite for config validation)
